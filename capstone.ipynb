{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnaLyrics Engine: Predicting Music Genres with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "This project leverages song lyrics and metadata to classify songs into distinct music genres. Genre classification is not an exact science, and given the vast diversity of music today, there are innumerable categories a song could belong to.\n",
    "\n",
    "As a passionate music lover and avid concert goer, I tend to have a curated playlist for every possible mood and occassion. This is a tedious task to do manually, and I have often thought how nice it would be to be able to input my current mood and desired music style (ex: sad bangers) and have a playlist made personally for me.\n",
    "\n",
    "Spotify has attempted to do this with their [daylist](https://newsroom.spotify.com/2023-09-12/ever-changing-playlist-daylist-music-for-all-day/)- a personalized playlist that \"ebbs and flows with unique vibes, bringing together the niche music and microgenres you usually listen to during particular moments in the day or on specific days of the week.\"\n",
    "\n",
    "The daylist updates organically throughout the day and is based on your past listening habits. This project attempts to build on that concept by curating a playlist based on a user's desired vibe. In order to get to an end goal of song recommendations based on lyrics and audio features, the first step was to build a supervised model trained to predict a song's genre based purely on the words it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "The first step in building the dataset was identifying the prediction target- music genres. As with any field that is an art and not a (data) science, the groundtruth for genre classification is fairly subjective.\n",
    "\n",
    "[Every Noise At Once](https://everynoise.com/engenremap.html) is â€œan ongoing attempt at an algorithmically-generated, readability-adjusted scatter-plot of the musical genre-space.\" The project was created in 2013 by Glenn McDonald, a former Spotify developer, based on data tracked and analyzed for ~6000 Spotify genres. \n",
    "\n",
    "![noise](images/noise.png)\n",
    "\n",
    "For the purposes of lyric classification, I chose to use the most popular distinct genre groupings- pop, rock, hip hop, and country- and the songs for each genre were collected from the playlists provided by the Every Noise project.\n",
    "\n",
    "Song metadata and audio features were gathered using [Spotipy](https://spotipy.readthedocs.io/en/2.22.1/), a Python wrapper for the [Spotify API](https://developer.spotify.com/documentation/web-api). Lyrics for each song were scraped from [Genius](https://genius.com) using [LyricsGenius](https://lyricsgenius.readthedocs.io/en/master/reference/genius.html). \n",
    "\n",
    "The scraped lyrics required preliminary text cleaning to remove extraneous content and bad records. After combining and cleaning the data, we were left with 2,024 songs for modeling.\n",
    "\n",
    "The Python files for collecting and cleaning the lyrics are available in the code folder:\n",
    "- [Spotify metadata](code/1_spotify.py)\n",
    "- [Genius lyrics](code/2_genius.py)\n",
    "- [Text cleaning](code/3_data_cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cleaned lyrics\n",
    "df = pd.read_csv('data/lyrics2.csv')\n",
    "df['genre'] = df['genre'].str.title()\n",
    "df['genre'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data, we performed preprocessing steps such as removing special characters and stop words, tokenization, and lemmatization of lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standardize text and lemmatize to get root terminology\n",
    "def preprocess_text(lyrics):\n",
    "    # remove numbers and special characters\n",
    "    lyrics = re.sub(r'[^a-zA-Z\\s]', '', lyrics)\n",
    "    # remove extra spaces and new lines\n",
    "    lyrics = re.sub(r'\\s+|\\n\\s*\\n', ' ', lyrics)\n",
    "    # lowercase all\n",
    "    lyrics = lyrics.lower()\n",
    "    # tokenize, lemmatize, remove stopwords\n",
    "    tokens = word_tokenize(lyrics)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    tokens = [lemmer.lemmatize(word) for word in tokens]\n",
    "    sw = stopwords.words('english')\n",
    "    tokens = ' '.join([word for word in tokens if word not in sw])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['lyrics_text'].apply(preprocess_text)\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring some simple modeling techniques, we found that there was heavy overfitting across the board- given this is a fairly small dataset with an average of 500 songs per genre, the models tended to perform much better on the training data than on unseen lyrics.\n",
    "\n",
    "Along with tuning the chosen models, we tested different parameters for vectorization and resampling, which only slightly improved overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cross validate model followed by train test split\n",
    "# generate classification report and confusion matrix\n",
    "\n",
    "def analyze_model(model, X, y):\n",
    "    # k-fold validation \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=95)\n",
    "    nb_kfold_scores = cross_validate(model, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "    print('Cross-Validation Results:')\n",
    "    for fold, (train_score, test_score) in enumerate(zip(nb_kfold_scores['train_score'], nb_kfold_scores['test_score'])):\n",
    "        print(f\"Fold {fold + 1}: Train = {train_score:.3f}, Test = {test_score:.3f}\")\n",
    "    print()\n",
    "    print(f\"Avg Train Accuracy: {nb_kfold_scores['train_score'].mean():.3f}\")\n",
    "    print(f\"Avg Test Accuracy: {nb_kfold_scores['test_score'].mean():.3f}\")\n",
    "    print()\n",
    "\n",
    "    # train test split for predictions\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=95)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_te)\n",
    "    genres = np.unique(y_te)\n",
    "\n",
    "    # classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_te, pred))\n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_te, pred, normalize='true')\n",
    "    sns.heatmap(cm, xticklabels=genres, yticklabels=genres,\n",
    "                annot=True, fmt='.2f', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split holdout set for final model validation\n",
    "\n",
    "X = df['tokens']\n",
    "y = df['genre']\n",
    "\n",
    "X_df, X_hold, y_df, y_hold = train_test_split(X, y, test_size=0.2, random_state=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X_df, y_df, test_size=0.2, random_state=95)\n",
    "\n",
    "# use dummy classifier as baseline model\n",
    "baseline = DummyClassifier(strategy='uniform')\n",
    "baseline.fit(X_tr, y_tr)\n",
    "base_pred = baseline.predict(X_te)\n",
    "base_score = accuracy_score(y_te, base_pred)\n",
    "print(f'Baseline Accuracy: {base_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple naive bayes \n",
    "\n",
    "nb_pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "]) \n",
    "\n",
    "analyze_model(nb_pipe, X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best parameters to address overfitting\n",
    "\n",
    "nb_params = {\n",
    "    'vect': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vect__max_features': [None, 200, 500],\n",
    "    'vect__min_df': [5, 10, 25],\n",
    "    'vect__max_df': [0.5, 0.75, 1.0],\n",
    "\n",
    "    'clf__alpha': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(nb_pipe, nb_params, cv=5, scoring='accuracy')\n",
    "nb_grid.fit(X_df, y_df)\n",
    "nb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned naive bayes\n",
    "\n",
    "nb_tuned = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=20, max_df=.75)),\n",
    "    ('clf', MultinomialNB(alpha=0.1))\n",
    "])\n",
    "\n",
    "analyze_model(nb_tuned, X_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple random forest\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "analyze_model(rf_pipe, X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best parameters to address overfitting\n",
    "\n",
    "rf_params = {\n",
    "    'vect': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vect__max_features': [None, 150, 600],\n",
    "    'vect__min_df': [2, 10, 25, 50],\n",
    "    'vect__max_df': [0.5, 0.75, 1.0],\n",
    "\n",
    "    'clf__n_estimators': [500, 600, 800],\n",
    "    'clf__max_depth': [20, 30, 50],\n",
    "    'clf__min_samples_split': [4, 6, 12],\n",
    "    'clf__min_samples_leaf': [2, 4, 7]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_df, y_df)\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned random forest\n",
    "\n",
    "rf_tuned = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df= 20, \n",
    "                             max_df= .75)),\n",
    "    ('clf', RandomForestClassifier(n_estimators= 500,\n",
    "                                   max_depth= 70,\n",
    "                                   min_samples_split= 3,\n",
    "                                   min_samples_leaf= 3))\n",
    "])\n",
    "\n",
    "analyze_model(rf_tuned, X_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In evaluation the final models, we found that a random forest provided the highest overall accuracy with 67%. However this may not be the preferred option as the accuracy of the classifications was heavily skewed in certain genres (rock and hip hop) compared to others. \n",
    "Naive Bayes overall accuracy dropped to 56%, but it was slightly more balanced across genres.\n",
    "\n",
    "To improve this model, we would need to collect a larger variety of lyrics to address sample size issues. We can also consider limiting the time frame as language evolution may be a factor in prediction accuracy.For expanded analysis, we can also combine audio features such as acoustics and tempo to focus on the song structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(nb_tuned, X_hold, y_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(rf_tuned, X_hold, y_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
