{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnaLyrics Engine: Predicting Music Genres with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This project leverages song lyrics and metadata to classify songs into distinct music genres.\n",
    "- The dataset is comprised of ~2000 songs distributed across 4 genres: pop, rock, hip-hop, and country.\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "The first step in building the dataset was identifying the prediction target- music genres. [Every Noise At Once](https://everynoise.com/engenremap.html) is â€œan ongoing attempt at an algorithmically-generated, readability-adjusted scatter-plot of the musical genre-space.\" The project was created in 2013 by Glenn McDonald, a former Spotify developer, based on data tracked and analyzed for ~6000 Spotify genres. (last updated November 2023)\n",
    "\n",
    "The songs for each genre were collected from the playlists provided by the Every Noise project, and all of the song metadata and audio features were gathered using [Spotipy](https://spotipy.readthedocs.io/en/2.22.1/), a Python wrapper for the [Spotify API](https://developer.spotify.com/documentation/web-api). Next, lyrics for each song were scraped from [Genius](https://genius.com) using [LyricsGenius](https://lyricsgenius.readthedocs.io/en/master/reference/genius.html).\n",
    "\n",
    "After combining all the data and dropping bad records, we were left with 2024 songs.\n",
    "\n",
    "We start by collecting data from Spotify, extracting features like acousticness, instrumentalness, and valence that capture the essence of each track. Additionally, we gather lyrics data from Genius, a platform known for its comprehensive collection of song lyrics. The combination of audio features and textual content provides a holistic representation of each song.\n",
    "\n",
    "Preprocessing:\n",
    "To prepare the data for analysis, we perform preprocessing steps such as text cleaning, tokenization, and vectorization of lyrics. This allows us to convert the textual content into a format suitable for NLP models.\n",
    "\n",
    "Feature Engineering:\n",
    "We engineer features from both Spotify and Genius data, creating a feature-rich dataset that encapsulates both the musical and lyrical aspects of each song. This combined feature set serves as the input for our genre classification model.\n",
    "\n",
    "Model Building:\n",
    "For genre classification, we employ machine learning models, such as Naive Bayes or Random Forests, trained on the labeled dataset. These models learn patterns from the features extracted from Spotify data and lyrics, enabling them to make predictions about the genre of a song.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "To enhance model performance, we use grid search and cross-validation techniques to fine-tune hyperparameters. This process ensures that our models generalize well to new, unseen data and minimizes the risk of overfitting.\n",
    "\n",
    "Evaluation:\n",
    "We evaluate the performance of our genre classification models using metrics like accuracy, precision, recall, and F1-score. This provides insights into how well the models are able to distinguish between different genres.\n",
    "\n",
    "Results and Insights:\n",
    "Upon successful model training and evaluation, we gain valuable insights into the factors influencing genre classification. We analyze the importance of various features and explore how different genres are characterized by both musical attributes and lyrical content.\n",
    "\n",
    "Through this project, we aim to contribute to the understanding of genre classification in the music domain, showcasing the potential of combining audio features and lyrics for accurate and insightful genre predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stopwords list\n",
    "sw = set(stopwords.words('english'))\n",
    "custom_sw = [\"i'd\", \"i'm\",\n",
    "             'yeah', 'ah', 'oh']\n",
    "sw.update(custom_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(lyrics):\n",
    "    # remove numbers and special characters\n",
    "    lyrics = re.sub(r'[^a-zA-Z\\s]', '', lyrics)\n",
    "    # remove extra spaces and new lines\n",
    "    lyrics = re.sub(r'\\s+|\\n\\s*\\n', ' ', lyrics)\n",
    "    # lowercase all\n",
    "    lyrics = lyrics.lower()\n",
    "    # tokenize, lemmatize, remove stopwords\n",
    "    tokens = word_tokenize(lyrics)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    tokens = [lemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = ' '.join([word for word in tokens if word not in sw])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       puttin defense cause dont wan na fall love eve...\n",
       "1       thats love thats love dont need time make mind...\n",
       "2       wake morning feelin like p diddy hey girl grab...\n",
       "3       good gold kinda dream cant sold right til were...\n",
       "4       found heart wa broke filled cup overflowed too...\n",
       "                              ...                        \n",
       "2019    saturday night six pack girl big star shining ...\n",
       "2020    oclock friday night im still home girl keep bl...\n",
       "2021    hard find perfect time say something know gon ...\n",
       "2022    tried get sober didnt get far im gon na pour c...\n",
       "2023    little outside elizabethtown little bar id sit...\n",
       "Name: tokens, Length: 2024, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['lyrics_text'].apply(preprocess_text)\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model, X, y):\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=95)\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_te)\n",
    "\n",
    "    # print(\"Classification Report:\")\n",
    "    # print(classification_report(y, pred))\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy', return_train_score=True) \n",
    "    print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean Accuracy: {cv_scores.mean():.2f}\")\n",
    "\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # cm = confusion_matrix(y, pred, normalize='true')\n",
    "    # genres = ['Pop', 'hip hop', 'rock', 'country']\n",
    "    # sns.heatmap(cm, xticklabels=genres, yticklabels=genres,\n",
    "    #             annot=True, fmt='.2f', cmap='Blues')\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('True')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rock       30.574429\n",
       "pop        26.065473\n",
       "country    24.336010\n",
       "hip hop    19.024089\n",
       "Name: genre, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set variables for modeling\n",
    "X = df['tokens']\n",
    "y = df['genre']\n",
    "\n",
    "# split holdout set for final model validation\n",
    "X_df, X_hold, y_df, y_hold = train_test_split(X, y, test_size=0.2, random_state=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X_df, y_df, test_size=0.2, random_state=95)\n",
    "\n",
    "# use dummy classifier as baseline model\n",
    "baseline = DummyClassifier(strategy='uniform')\n",
    "baseline.fit(X_tr, y_tr)\n",
    "base_pred = baseline.predict(X_te)\n",
    "base_score = accuracy_score(y_te, base_pred)\n",
    "print(f'Baseline Accuracy: {base_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.58641975 0.61111111 0.61111111 0.58641975 0.6006192 ]\n",
      "Mean Accuracy: 0.60\n",
      "Overall Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "nb_pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "]) \n",
    "\n",
    "analyze_model(nb_pipe, X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.1,\n",
       " 'vect': CountVectorizer(),\n",
       " 'vect__max_df': 0.75,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 2}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_params = {\n",
    "    'vect': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vect__max_features': [None, 1000, 2000],\n",
    "    'vect__min_df': [2, 10, 25,50],\n",
    "    'vect__max_df': [0.5, 0.75, 1.0],\n",
    "\n",
    "    'clf__alpha': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(nb_pipe, nb_params, cv=5, scoring='accuracy')\n",
    "nb_grid.fit(X_df, y_df)\n",
    "print('Naive Bayes Best Params: ', nb_grid.best_params_)\n",
    "nb_tuned = nb_grid.best_estimator_\n",
    "nb_pred = nb_tuned.predict(X_te)\n",
    "nb_score = accuracy_score(y_te, nb_pred)\n",
    "print(f'Naive Bayes Accuracy: {nb_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.91\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.62654321 0.64506173 0.66358025 0.61111111 0.66873065]\n",
      "Mean Accuracy: 0.64\n",
      "Overall Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "analyze_model(rf_pipe, X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'vect': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vect__max_features': [None, 1000, 2000],\n",
    "    'vect__min_df': [2, 10, 25,50],\n",
    "    'vect__max_df': [0.5, 0.75, 1.0],\n",
    "\n",
    "    'clf__n_estimators': [50, 100, 150],\n",
    "    'clf__max_depth': [None, 10, 20, 30],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_df, y_df)\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Best Params: ', rf_grid.best_params_)\n",
    "rf_tuned = rf_grid.best_estimator_\n",
    "rf_pred = rf_tuned.predict(X_te)\n",
    "rf_score = accuracy_score(y_te, rf_pred)\n",
    "print(f'Random Forest Accuracy: {rf_score:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
